{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1956405,"sourceType":"datasetVersion","datasetId":1060121}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 02_explore_data_folder\n!ls -la /kaggle/input/goemotions/data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:45:08.162593Z","iopub.execute_input":"2025-10-26T14:45:08.163199Z","iopub.status.idle":"2025-10-26T14:45:08.296172Z","shell.execute_reply.started":"2025-10-26T14:45:08.163175Z","shell.execute_reply":"2025-10-26T14:45:08.295236Z"}},"outputs":[{"name":"stdout","text":"total 4320\ndrwxr-xr-x 3 nobody nogroup       0 Oct 25 17:08 .\ndrwxr-xr-x 5 nobody nogroup       0 Oct 25 17:08 ..\n-rw-r--r-- 1 nobody nogroup  439059 Oct 25 17:08 dev.tsv\n-rw-r--r-- 1 nobody nogroup      55 Oct 25 17:08 ekman_labels.csv\n-rw-r--r-- 1 nobody nogroup     396 Oct 25 17:08 ekman_mapping.json\n-rw-r--r-- 1 nobody nogroup     248 Oct 25 17:08 emotions.txt\ndrwxr-xr-x 2 nobody nogroup       0 Oct 25 17:08 full_dataset\n-rw-r--r-- 1 nobody nogroup     367 Oct 25 17:08 sentiment_dict.json\n-rw-r--r-- 1 nobody nogroup     367 Oct 25 17:08 sentiment_mapping.json\n-rw-r--r-- 1 nobody nogroup  436706 Oct 25 17:08 test.tsv\n-rw-r--r-- 1 nobody nogroup 3519053 Oct 25 17:08 train.tsv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 03_load_data_to_pandas\nfrom pathlib import Path\nimport pandas as pd\n\nDATA_ROOT = Path(\"/kaggle/input/goemotions/data\")\n\ncols = [\"text\", \"labels\", \"id\"]\n\ntrain = pd.read_csv(DATA_ROOT / \"train.tsv\", sep=\"\\t\", names=cols, header=None, quoting=3)\ndev   = pd.read_csv(DATA_ROOT / \"dev.tsv\",   sep=\"\\t\", names=cols, header=None, quoting=3)\ntest  = pd.read_csv(DATA_ROOT / \"test.tsv\",  sep=\"\\t\", names=cols, header=None, quoting=3)\n\nprint(f\"Train: {len(train)} | Dev: {len(dev)} | Test: {len(test)}\")\ntrain.head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:45:47.729322Z","iopub.execute_input":"2025-10-26T14:45:47.730133Z","iopub.status.idle":"2025-10-26T14:45:48.243617Z","shell.execute_reply.started":"2025-10-26T14:45:47.730104Z","shell.execute_reply":"2025-10-26T14:45:48.242810Z"}},"outputs":[{"name":"stdout","text":"Train: 43410 | Dev: 5426 | Test: 5427\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                text labels       id\n0  My favourite food is anything I didn't have to...     27  eebbqej\n1  Now if he does off himself, everyone will thin...     27  ed00q6i\n2                     WHY THE FUCK IS BAYLESS ISOING      2  eezlygj\n3                        To make her feel threatened     14  ed7ypvh\n4                             Dirty Southern Wankers      3  ed0bdzj","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My favourite food is anything I didn't have to...</td>\n      <td>27</td>\n      <td>eebbqej</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now if he does off himself, everyone will thin...</td>\n      <td>27</td>\n      <td>ed00q6i</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n      <td>2</td>\n      <td>eezlygj</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>To make her feel threatened</td>\n      <td>14</td>\n      <td>ed7ypvh</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dirty Southern Wankers</td>\n      <td>3</td>\n      <td>ed0bdzj</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# 04_label_preprocessing_multihot\nfrom pathlib import Path\nimport numpy as np\n\nEMOTIONS_FP = Path(\"/kaggle/input/goemotions/data/emotions.txt\")\nwith open(EMOTIONS_FP, \"r\") as f:\n    emotions = [s.strip() for s in f if s.strip()]\nNUM_LABELS = len(emotions)  # expect 28 (27 emotions + neutral)\n\nprint(\"NUM_LABELS =\", NUM_LABELS)\nprint(\"first 10 labels:\", emotions[:10])\n\n# convert comma-separated label ids -> multihot vector\ndef labels_to_multihot(label_str, n=NUM_LABELS):\n    if pd.isna(label_str) or str(label_str).strip() == \"\":\n        return np.zeros(n, dtype=int)\n    ids = [int(x) for x in str(label_str).split(\",\") if x != \"\"]\n    vec = np.zeros(n, dtype=int)\n    vec[ids] = 1\n    return vec\n\ntrain[\"label_vec\"] = train[\"labels\"].apply(labels_to_multihot)\ndev[\"label_vec\"]   = dev[\"labels\"].apply(labels_to_multihot)\ntest[\"label_vec\"]  = test[\"labels\"].apply(labels_to_multihot)\n\n# quick sanity checks\nprint(\"Example row (text, labels, multihot):\")\nprint(train.iloc[0][[\"text\",\"labels\",\"label_vec\"]])\nprint(\"\\nLabel counts (how many examples include each label):\")\nlabel_counts = np.sum(np.stack(train[\"label_vec\"].values), axis=0)\nfor i, (lbl, cnt) in enumerate(zip(emotions, label_counts)):\n    print(f\"{i:02d} {lbl:20.20}  {int(cnt)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:46:52.311039Z","iopub.execute_input":"2025-10-26T14:46:52.311662Z","iopub.status.idle":"2025-10-26T14:46:52.522763Z","shell.execute_reply.started":"2025-10-26T14:46:52.311636Z","shell.execute_reply":"2025-10-26T14:46:52.522090Z"}},"outputs":[{"name":"stdout","text":"NUM_LABELS = 28\nfirst 10 labels: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment']\nExample row (text, labels, multihot):\ntext         My favourite food is anything I didn't have to...\nlabels                                                      27\nlabel_vec    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nName: 0, dtype: object\n\nLabel counts (how many examples include each label):\n00 admiration            4130\n01 amusement             2328\n02 anger                 1567\n03 annoyance             2470\n04 approval              2939\n05 caring                1087\n06 confusion             1368\n07 curiosity             2191\n08 desire                641\n09 disappointment        1269\n10 disapproval           2022\n11 disgust               793\n12 embarrassment         303\n13 excitement            853\n14 fear                  596\n15 gratitude             2662\n16 grief                 77\n17 joy                   1452\n18 love                  2086\n19 nervousness           164\n20 optimism              1581\n21 pride                 111\n22 realization           1110\n23 relief                153\n24 remorse               545\n25 sadness               1326\n26 surprise              1060\n27 neutral               14219\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 05_small_debug_subset\n# Create a tiny subset to test the pipeline quickly (keeps first runs short)\nDEBUG = True\n\nif DEBUG:\n    quick_train = train.sample(min(1000, len(train)), random_state=42).reset_index(drop=True)\n    quick_dev   = dev.sample(min(200, len(dev)), random_state=42).reset_index(drop=True)\n    quick_test  = test.sample(min(500, len(test)), random_state=1).reset_index(drop=True)\nelse:\n    quick_train, quick_dev, quick_test = train, dev, test\n\nprint(\"DEBUG =\", DEBUG)\nprint(\"quick sizes -> train:\", len(quick_train), \"dev:\", len(quick_dev), \"test:\", len(quick_test))\nquick_train.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:48:02.136668Z","iopub.execute_input":"2025-10-26T14:48:02.137348Z","iopub.status.idle":"2025-10-26T14:48:02.155304Z","shell.execute_reply.started":"2025-10-26T14:48:02.137323Z","shell.execute_reply":"2025-10-26T14:48:02.154672Z"}},"outputs":[{"name":"stdout","text":"DEBUG = True\nquick sizes -> train: 1000 dev: 200 test: 500\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                text labels       id  \\\n0  The only way this works is if [NAME] is doing ...     27  edupnyh   \n1  Access should be hindered it's getting destroyed.      3  ediy7lp   \n2  Totally fair. All I was trying to remind every...      4  edv791a   \n\n                                           label_vec  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n1  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n      <th>id</th>\n      <th>label_vec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The only way this works is if [NAME] is doing ...</td>\n      <td>27</td>\n      <td>edupnyh</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Access should be hindered it's getting destroyed.</td>\n      <td>3</td>\n      <td>ediy7lp</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Totally fair. All I was trying to remind every...</td>\n      <td>4</td>\n      <td>edv791a</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 06_install_and_import_hf\n# Run this once at the top of the notebook to ensure required packages are available.\n# It may take ~1-2 minutes the first time.\n!pip install -q transformers datasets accelerate evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:48:38.858485Z","iopub.execute_input":"2025-10-26T14:48:38.858977Z","iopub.status.idle":"2025-10-26T14:49:58.113964Z","shell.execute_reply.started":"2025-10-26T14:48:38.858957Z","shell.execute_reply":"2025-10-26T14:49:58.113031Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 07b_tokenize_with_torchdataset\n# Tokenize using the HF tokenizer and build torch.utils.data.Dataset objects\n# This avoids the `datasets` library (so no pyarrow required).\n\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\n\nMODEL_NAME = \"distilroberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nmax_length = 30\n\nclass GoEmotionsTorchDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, label_vecs, tokenizer, max_length=30):\n        # texts: pandas Series of strings\n        # label_vecs: pandas Series of numpy arrays or lists\n        self.tokenized = tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length,\n            return_tensors=\"pt\",\n        )\n        # stack labels into (N, num_labels) float tensor for BCE\n        self.labels = torch.tensor(np.stack(label_vecs.values), dtype=torch.float)\n        assert self.labels.shape[0] == self.tokenized[\"input_ids\"].shape[0], \"mismatch lengths\"\n\n    def __len__(self):\n        return self.labels.shape[0]\n\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.tokenized.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\n# Build datasets from the quick_... pandas DataFrames you already created\ntrain_dataset_torch = GoEmotionsTorchDataset(quick_train[\"text\"], quick_train[\"label_vec\"], tokenizer, max_length=max_length)\ndev_dataset_torch   = GoEmotionsTorchDataset(quick_dev[\"text\"], quick_dev[\"label_vec\"], tokenizer, max_length=max_length)\ntest_dataset_torch  = GoEmotionsTorchDataset(quick_test[\"text\"], quick_test[\"label_vec\"], tokenizer, max_length=max_length)\n\n# quick sanity prints\nprint(\"Train size:\", len(train_dataset_torch))\nprint(\"Dev size:\", len(dev_dataset_torch))\nprint(\"Test size:\", len(test_dataset_torch))\n\nsample = train_dataset_torch[0]\nprint(\"sample keys:\", list(sample.keys()))\nprint(\"input_ids length:\", sample[\"input_ids\"].shape, \"labels shape:\", sample[\"labels\"].shape)\nprint(\"first 10 input_ids:\", sample[\"input_ids\"][:10].tolist())\nprint(\"first label vector indices set:\", torch.where(sample[\"labels\"] == 1)[0].tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:59:44.675942Z","iopub.execute_input":"2025-10-26T14:59:44.676190Z","iopub.status.idle":"2025-10-26T14:59:44.935224Z","shell.execute_reply.started":"2025-10-26T14:59:44.676172Z","shell.execute_reply":"2025-10-26T14:59:44.934409Z"}},"outputs":[{"name":"stdout","text":"Train size: 1000\nDev size: 200\nTest size: 500\nsample keys: ['input_ids', 'attention_mask', 'labels']\ninput_ids length: torch.Size([30]) labels shape: torch.Size([28])\nfirst 10 input_ids: [0, 133, 129, 169, 42, 1364, 16, 114, 646, 48307]\nfirst label vector indices set: [27]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 08_manual_train_multiGPU\nimport os\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom sklearn.metrics import f1_score\nfrom tqdm.auto import tqdm\n\n# Config (tweak if OOM)\nMODEL_NAME = \"distilroberta-base\"\nNUM_LABELS = 28\nOUTDIR = \"/kaggle/working/goemotions-checkpoint-manual\"\nos.makedirs(OUTDIR, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nngpu = torch.cuda.device_count()\nprint(\"Device:\", device, \"GPUs:\", ngpu)\nfor i in range(ngpu):\n    print(i, torch.cuda.get_device_name(i))\n\n# load model/tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS, problem_type=\"multi_label_classification\")\nif ngpu > 1:\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)\n\n# training hyperparams\nepochs = 3\nper_device_batch_size = 16   # if OOM reduce to 8 or 4\ngradient_accumulation_steps = 2\neffective_batch = per_device_batch_size * gradient_accumulation_steps * max(1, ngpu)\nprint(\"Effective batch (approx):\", effective_batch)\n\nlr = 3e-5\nweight_decay = 0.01\n\n# dataloaders\ntrain_loader = DataLoader(train_dataset_torch, batch_size=per_device_batch_size, shuffle=True, num_workers=2)\ndev_loader   = DataLoader(dev_dataset_torch,   batch_size=per_device_batch_size*2, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_dataset_torch,  batch_size=per_device_batch_size*2, shuffle=False, num_workers=2)\n\n# optimizer & loss\noptimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\ncriterion = torch.nn.BCEWithLogitsLoss()\n\n# amp scaler for fp16\nscaler = torch.cuda.amp.GradScaler(enabled=True)\n\nbest_val_f1 = -1.0\n\ndef evaluate(loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in loader:\n            inputs = {k: v.to(device) for k,v in batch.items() if k in [\"input_ids\",\"attention_mask\"]}\n            labels = batch[\"labels\"].to(device)\n            with torch.cuda.amp.autocast(enabled=True):\n                outputs = model(**inputs)\n                logits = outputs.logits if not isinstance(outputs, tuple) else outputs[0]\n            probs = torch.sigmoid(logits).cpu().numpy()\n            preds = (probs >= 0.5).astype(int)\n            all_preds.append(preds)\n            all_labels.append(labels.cpu().numpy().astype(int))\n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    micro = f1_score(all_labels.reshape(-1), all_preds.reshape(-1), average=\"micro\", zero_division=0)\n    return micro, all_preds, all_labels\n\n# training loop\nglobal_step = 0\nfor epoch in range(1, epochs+1):\n    model.train()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    running_loss = 0.0\n    optimizer.zero_grad()\n    for step, batch in enumerate(pbar, start=1):\n        inputs = {k: v.to(device) for k,v in batch.items() if k in [\"input_ids\",\"attention_mask\"]}\n        labels = batch[\"labels\"].to(device)\n\n        with torch.cuda.amp.autocast(enabled=True):\n            outputs = model(**inputs, labels=None)\n            logits = outputs.logits if not isinstance(outputs, tuple) else outputs[0]\n            loss = criterion(logits, labels)\n\n        scaler.scale(loss / gradient_accumulation_steps).backward()\n        running_loss += loss.item()\n\n        if (step % gradient_accumulation_steps) == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            pbar.set_postfix({\"loss\": f\"{running_loss/(global_step):.4f}\"})\n\n    # end epoch: evaluate\n    val_micro, _, _ = evaluate(dev_loader)\n    print(f\"\\nEpoch {epoch} validation micro-F1: {val_micro:.4f}\")\n\n    # save best\n    if val_micro > best_val_f1:\n        best_val_f1 = val_micro\n        # if DataParallel, unwrap\n        model_to_save = model.module if hasattr(model, \"module\") else model\n        model_to_save.save_pretrained(OUTDIR)\n        tokenizer.save_pretrained(OUTDIR)\n        print(f\"Saved best model to {OUTDIR} (micro-F1={best_val_f1:.4f})\")\n\n# final test eval\ntest_micro, test_preds, test_labels = evaluate(test_loader)\nprint(\"Final test micro-F1:\", test_micro)\n# save predictions (small file)\nnp.savez_compressed(\"/kaggle/working/goemotions_preds.npz\", preds=test_preds, labels=test_labels)\nprint(\"Saved predictions to /kaggle/working/goemotions_preds.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:02:02.609711Z","iopub.execute_input":"2025-10-26T15:02:02.610030Z","iopub.status.idle":"2025-10-26T15:02:03.768400Z","shell.execute_reply.started":"2025-10-26T15:02:02.610003Z","shell.execute_reply":"2025-10-26T15:02:03.767363Z"}},"outputs":[{"name":"stdout","text":"Device: cuda GPUs: 2\n0 Tesla T4\n1 Tesla T4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcapture_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoggingTensorMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchDispatchMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/testing/_internal/logging_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_abbrs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtype_abbrs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_dtype_abbrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchDispatchMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_dtype_abbrs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_e5m2fnuz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"f8e5m2fnuz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_e8m0fnu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"f8e8m0fnu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat4_e2m1fn_x2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"f4e2m1fnx2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2681\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2682\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'float8_e8m0fnu'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3546779152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# load model/tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mngpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m             )\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             return model_class.from_pretrained(\n\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1513\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.roberta.modeling_roberta because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'float8_e8m0fnu'"],"ename":"RuntimeError","evalue":"Failed to import transformers.models.roberta.modeling_roberta because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'float8_e8m0fnu'","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"# run this cell first and paste the output if you want me to review it\nimport torch, sys\nprint(\"python:\", sys.version.split()[0])\nprint(\"torch:\", getattr(torch, \"__version__\", None))\nprint(\"torch.cuda:\", torch.version.cuda)\nprint(\"cuda available:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:03:00.332827Z","iopub.execute_input":"2025-10-26T15:03:00.333104Z","iopub.status.idle":"2025-10-26T15:03:00.338096Z","shell.execute_reply.started":"2025-10-26T15:03:00.333085Z","shell.execute_reply":"2025-10-26T15:03:00.337338Z"}},"outputs":[{"name":"stdout","text":"python: 3.11.13\ntorch: 2.6.0+cu124\ntorch.cuda: 12.4\ncuda available: True\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# 09_train_tfidf_baseline\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\nimport joblib\nfrom tqdm.auto import tqdm\n\n# prepare data from quick_train/quick_dev/quick_test (pandas DataFrames)\nX_train = quick_train[\"text\"].astype(str).tolist()\ny_train = np.stack(quick_train[\"label_vec\"].values).astype(int)\n\nX_dev = quick_dev[\"text\"].astype(str).tolist()\ny_dev = np.stack(quick_dev[\"label_vec\"].values).astype(int)\n\nX_test = quick_test[\"text\"].astype(str).tolist()\ny_test = np.stack(quick_test[\"label_vec\"].values).astype(int)\n\nprint(\"Sizes:\", len(X_train), len(X_dev), len(X_test))\n\n# pipeline: TF-IDF -> OneVsRest(LogisticRegression)\npipeline = Pipeline([\n    (\"tfidf\", TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=2)),\n    (\"clf\", OneVsRestClassifier(LogisticRegression(solver=\"saga\", max_iter=200, C=1.0, n_jobs=-1)))\n])\n\n# train (fast)\npipeline.fit(X_train, y_train)\n\n# predict dev + test\ny_dev_pred = pipeline.predict(X_dev)\ny_test_pred = pipeline.predict(X_test)\n\n# metrics (micro & macro, flatten for multi-label as in earlier)\ndev_micro = f1_score(y_dev.reshape(-1), y_dev_pred.reshape(-1), average=\"micro\", zero_division=0)\ndev_macro = f1_score(y_dev.reshape(-1), y_dev_pred.reshape(-1), average=\"macro\", zero_division=0)\ntest_micro = f1_score(y_test.reshape(-1), y_test_pred.reshape(-1), average=\"micro\", zero_division=0)\ntest_macro = f1_score(y_test.reshape(-1), y_test_pred.reshape(-1), average=\"macro\", zero_division=0)\n\nprint(f\"Dev micro-F1: {dev_micro:.4f} | Dev macro-F1: {dev_macro:.4f}\")\nprint(f\"Test micro-F1: {test_micro:.4f} | Test macro-F1: {test_macro:.4f}\")\n\n# show a few examples\ndef show_examples(X, y_true, y_pred, n=6):\n    inv = {i: lab for i, lab in enumerate(emotions)}\n    for i in range(min(n, len(X))):\n        true_idx = np.where(y_true[i]==1)[0].tolist()\n        pred_idx = np.where(y_pred[i]==1)[0].tolist()\n        print(f\"TEXT: {X[i][:200]!s}\")\n        print(\"TRUE:\", [inv[j] for j in true_idx])\n        print(\"PRED:\", [inv[j] for j in pred_idx])\n        print(\"-\"*60)\n\nprint(\"\\nSample dev predictions:\")\nshow_examples(X_dev, y_dev, y_dev_pred, n=6)\n\n# persist model & vectorizer\njoblib.dump(pipeline, \"/kaggle/working/goemotions_tfidf_baseline.joblib\")\nprint(\"Saved baseline to /kaggle/working/goemotions_tfidf_baseline.joblib\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:04:42.529430Z","iopub.execute_input":"2025-10-26T15:04:42.530200Z","iopub.status.idle":"2025-10-26T15:04:44.431621Z","shell.execute_reply.started":"2025-10-26T15:04:42.530176Z","shell.execute_reply":"2025-10-26T15:04:44.430805Z"}},"outputs":[{"name":"stdout","text":"Sizes: 1000 200 500\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Dev micro-F1: 0.9595 | Dev macro-F1: 0.5264\nTest micro-F1: 0.9582 | Test macro-F1: 0.5272\n\nSample dev predictions:\nTEXT: Haha, my apologies!\nTRUE: ['amusement']\nPRED: []\n------------------------------------------------------------\nTEXT: It surprises me that he's a mod some days...\nTRUE: ['surprise']\nPRED: []\n------------------------------------------------------------\nTEXT: Seems to be fake. Checked the Morning Mix website, no articles from Nov 13^(th) 2018 that match the title shown.\nTRUE: ['neutral']\nPRED: []\n------------------------------------------------------------\nTEXT: I have faith\nTRUE: ['neutral']\nPRED: []\n------------------------------------------------------------\nTEXT: Oh. :[\nTRUE: ['neutral']\nPRED: []\n------------------------------------------------------------\nTEXT: Tbh I love [NAME] but for [NAME] id be willing to see him go. [NAME] gives [NAME] the best chance he'll ever have of competing.\nTRUE: ['neutral']\nPRED: []\n------------------------------------------------------------\nSaved baseline to /kaggle/working/goemotions_tfidf_baseline.joblib\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# diag_1_label_mapping_and_stats\nimport numpy as np\ninv = {i: lab for i, lab in enumerate(emotions)}\nprint(\"Label index for 'neutral' check:\", [(i, emotions[i]) for i in range(len(emotions)) if 'neutral' in emotions[i].lower()])\n\n# show how many positives model predicted per dev sample and per label\nprint(\"Per-sample predicted positives (first 20):\", np.sum(y_dev_pred, axis=1)[:20])\nprint(\"Histogram of positives per sample:\", np.bincount(np.sum(y_dev_pred, axis=1).astype(int))[:10])\npred_counts_per_label = np.sum(y_dev_pred, axis=0)\ntrue_counts_per_label = np.sum(y_dev, axis=0)\nfor i,cnt_t,cnt_p in zip(range(len(emotions)), true_counts_per_label, pred_counts_per_label):\n    if i < 10 or cnt_t < 50 or cnt_p < 50:  # print many small/interesting ones\n        print(f\"{i:02d} {inv[i]:15.15} true:{int(cnt_t):5d} pred:{int(cnt_p):5d}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:06:38.792174Z","iopub.execute_input":"2025-10-26T15:06:38.792830Z","iopub.status.idle":"2025-10-26T15:06:38.800755Z","shell.execute_reply.started":"2025-10-26T15:06:38.792806Z","shell.execute_reply":"2025-10-26T15:06:38.800081Z"}},"outputs":[{"name":"stdout","text":"Label index for 'neutral' check: [(27, 'neutral')]\nPer-sample predicted positives (first 20): [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\nHistogram of positives per sample: [190  10]\n00 admiration      true:   16 pred:    1\n01 amusement       true:    3 pred:    1\n02 anger           true:   13 pred:    0\n03 annoyance       true:   12 pred:    0\n04 approval        true:   15 pred:    0\n05 caring          true:    5 pred:    0\n06 confusion       true:    6 pred:    0\n07 curiosity       true:   10 pred:    0\n08 desire          true:    1 pred:    0\n09 disappointment  true:    8 pred:    0\n10 disapproval     true:   14 pred:    0\n11 disgust         true:    3 pred:    0\n12 embarrassment   true:    1 pred:    0\n13 excitement      true:    2 pred:    0\n14 fear            true:    1 pred:    0\n15 gratitude       true:   11 pred:    1\n16 grief           true:    0 pred:    0\n17 joy             true:    6 pred:    0\n18 love            true:   11 pred:    1\n19 nervousness     true:    1 pred:    0\n20 optimism        true:   12 pred:    0\n21 pride           true:    0 pred:    0\n22 realization     true:    5 pred:    0\n23 relief          true:    0 pred:    0\n24 remorse         true:    0 pred:    0\n25 sadness         true:    6 pred:    0\n26 surprise        true:    9 pred:    0\n27 neutral         true:   64 pred:    6\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# diag_2_example_probs_and_features\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# access pipeline parts (pipeline: TfidfVectorizer + OneVsRestClassifier)\ntfidf = pipeline.named_steps['tfidf']\nclf = pipeline.named_steps['clf']  # OneVsRestClassifier\n\n# find index of example in dev set (first matching substring)\nidx = None\nfor i, txt in enumerate(X_dev):\n    if \"have faith\" in txt.lower():\n        idx = i\n        break\nprint(\"example idx in dev:\", idx, \"text:\", X_dev[idx])\n\n# TF-IDF sparsity info\nvec = tfidf.transform([X_dev[idx]])\nnz = vec.count_nonzero()\nprint(\"TF-IDF non-zero features for this example:\", nz)\n\n# per-label decision function / probabilities\n# Many estimators expose decision_function; if not, use predict_proba\nprobas = None\ntry:\n    # OneVsRestClassifier has estimators_ list\n    # use predict_proba if available\n    probas = clf.predict_proba([X_dev[idx]])  # shape (1, n_labels)\nexcept Exception:\n    # fallback to decision_function then convert via sigmoid\n    from scipy.special import expit\n    dec = np.array([est.decision_function(tfidf.transform([X_dev[idx]])) for est in clf.estimators_]).squeeze().T\n    probas = expit(dec)\n\nprobas = np.array(probas).reshape(-1)\ntopk = probas.argsort()[::-1][:6]\nprint(\"Top probabilities (label idx:label:prob):\")\nfor i in topk:\n    print(i, inv[i], f\"{probas[i]:.4f}\")\nprint(\"Probability for 'neutral' index (if found):\", \n      [(i, inv[i], float(probas[i])) for i in range(len(emotions)) if 'neutral' in inv[i].lower()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:06:59.631994Z","iopub.execute_input":"2025-10-26T15:06:59.632753Z","iopub.status.idle":"2025-10-26T15:06:59.662957Z","shell.execute_reply.started":"2025-10-26T15:06:59.632726Z","shell.execute_reply":"2025-10-26T15:06:59.662190Z"}},"outputs":[{"name":"stdout","text":"example idx in dev: 3 text: I have faith\nTF-IDF non-zero features for this example: 1\nTop probabilities (label idx:label:prob):\n27 neutral 0.3349\n0 admiration 0.0694\n3 annoyance 0.0597\n7 curiosity 0.0533\n1 amusement 0.0498\n15 gratitude 0.0493\nProbability for 'neutral' index (if found): [(27, 'neutral', 0.33494177392167707)]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# fix_A_lower_threshold_quick\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# clf, tfidf, X_dev, y_dev were defined earlier in the baseline cell\n# get dev probabilities (shape n_samples x n_labels)\ntry:\n    dev_probas = clf.predict_proba(X_dev)\nexcept Exception:\n    # fallback if predict_proba not available\n    from scipy.special import expit\n    dec = np.vstack([est.decision_function(tfidf.transform(X_dev)) for est in clf.estimators_]).T\n    dev_probas = expit(dec)\n\nfor thr in [0.5, 0.4, 0.35, 0.3, 0.25, 0.2]:\n    preds_thr = (dev_probas >= thr).astype(int)\n    micro = f1_score(y_dev.reshape(-1), preds_thr.reshape(-1), average=\"micro\", zero_division=0)\n    macro = f1_score(y_dev.reshape(-1), preds_thr.reshape(-1), average=\"macro\", zero_division=0)\n    avg_pos = preds_thr.sum()/preds_thr.shape[0]\n    print(f\"thr={thr:.2f} -> dev micro-F1: {micro:.4f} | macro-F1: {macro:.4f} | avg positives/sample: {avg_pos:.3f}\")\n\n# show the example prediction for \"I have faith\" with chosen threshold\nexample_idx = idx  # earlier you found idx for \"I have faith\"\nchosen_thr = 0.3\nprobas_example = dev_probas[example_idx]\npreds_example = np.where(probas_example >= chosen_thr)[0].tolist()\nprint(\"probas (top6):\", sorted([(i, emotions[i], float(probas_example[i])) for i in range(len(probas_example))], key=lambda x: -x[2])[:6])\nprint(\"predicted label indices @ thr\", chosen_thr, \":\", preds_example)\nprint(\"predicted label names:\", [emotions[i] for i in preds_example])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:08:57.576592Z","iopub.execute_input":"2025-10-26T15:08:57.577191Z","iopub.status.idle":"2025-10-26T15:08:57.729426Z","shell.execute_reply.started":"2025-10-26T15:08:57.577169Z","shell.execute_reply":"2025-10-26T15:08:57.728473Z"}},"outputs":[{"name":"stdout","text":"thr=0.50 -> dev micro-F1: 0.9595 | macro-F1: 0.5264 | avg positives/sample: 0.050\nthr=0.40 -> dev micro-F1: 0.9602 | macro-F1: 0.6066 | avg positives/sample: 0.280\nthr=0.35 -> dev micro-F1: 0.9591 | macro-F1: 0.6350 | avg positives/sample: 0.440\nthr=0.30 -> dev micro-F1: 0.9568 | macro-F1: 0.6546 | avg positives/sample: 0.635\nthr=0.25 -> dev micro-F1: 0.9534 | macro-F1: 0.6575 | avg positives/sample: 0.800\nthr=0.20 -> dev micro-F1: 0.9509 | macro-F1: 0.6591 | avg positives/sample: 0.920\nprobas (top6): [(27, 'neutral', 0.33494177392167707), (0, 'admiration', 0.06944683408539752), (3, 'annoyance', 0.05973364217308284), (7, 'curiosity', 0.0533066501290191), (1, 'amusement', 0.049826234031810746), (15, 'gratitude', 0.049270827739021514)]\npredicted label indices @ thr 0.3 : [27]\npredicted label names: ['neutral']\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# fix_B_retrain_vectorizer_more_features\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score\nimport joblib\n\npipeline2 = Pipeline([\n    (\"tfidf\", TfidfVectorizer(max_features=40000, ngram_range=(1,3), min_df=1, analyzer=\"word\")),\n    # consider adding char ngrams: uncomment next line and comment previous tfidf if desired\n    # (\"tfidf\", TfidfVectorizer(max_features=40000, ngram_range=(1,3), min_df=1, analyzer=\"char_wb\")),\n    (\"clf\", OneVsRestClassifier(LogisticRegression(solver=\"saga\", max_iter=1000, C=1.0, n_jobs=-1)))\n])\n\npipeline2.fit(X_train, y_train)\n\n# evaluate on dev\ny_dev_pred2 = pipeline2.predict(X_dev)\ndev_micro2 = f1_score(y_dev.reshape(-1), y_dev_pred2.reshape(-1), average=\"micro\", zero_division=0)\ndev_macro2 = f1_score(y_dev.reshape(-1), y_dev_pred2.reshape(-1), average=\"macro\", zero_division=0)\nprint(\"New pipeline dev micro-F1:\", dev_micro2, \"macro-F1:\", dev_macro2)\n\n# save\njoblib.dump(pipeline2, \"/kaggle/working/goemotions_tfidf_pipeline2.joblib\")\nprint(\"Saved improved pipeline to /kaggle/working/goemotions_tfidf_pipeline2.joblib\")\n\n# sample the example again\ntfidf2 = pipeline2.named_steps[\"tfidf\"]\nclf2 = pipeline2.named_steps[\"clf\"]\ntry:\n    dev_probas2 = clf2.predict_proba(X_dev)\nexcept Exception:\n    from scipy.special import expit\n    dec2 = np.vstack([est.decision_function(tfidf2.transform(X_dev)) for est in clf2.estimators_]).T\n    dev_probas2 = expit(dec2)\n\nprint(\"Example nonzero features:\", tfidf2.transform([X_dev[idx]]).count_nonzero())\nprint(\"Neutral prob for example:\", float(dev_probas2[idx, emotions.index(\"neutral\")]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:09:04.712901Z","iopub.execute_input":"2025-10-26T15:09:04.713636Z","iopub.status.idle":"2025-10-26T15:09:11.875939Z","shell.execute_reply.started":"2025-10-26T15:09:04.713611Z","shell.execute_reply":"2025-10-26T15:09:11.875225Z"}},"outputs":[{"name":"stdout","text":"New pipeline dev micro-F1: 0.9582142857142857 macro-F1: 0.493566000284444\nSaved improved pipeline to /kaggle/working/goemotions_tfidf_pipeline2.joblib\nExample nonzero features: 2\nNeutral prob for example: 0.3627720442782259\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# 10_compute_per_label_thresholds_and_eval\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport joblib\n\n# Use pipeline2 if you re-trained; otherwise use pipeline & dev_probas from earlier.\n# I'll prefer pipeline2 if it exists, fallback to pipeline.\nclf_used = None\ntfidf_used = None\ntry:\n    pipeline2  # exists if you ran the improved pipeline\n    clf_used = pipeline2.named_steps[\"clf\"]\n    tfidf_used = pipeline2.named_steps[\"tfidf\"]\n    print(\"Using pipeline2 (improved).\")\nexcept NameError:\n    clf_used = pipeline.named_steps[\"clf\"]\n    tfidf_used = pipeline.named_steps[\"tfidf\"]\n    print(\"Using original pipeline.\")\n\n# compute dev probabilities (n_samples x n_labels)\ntry:\n    dev_probas = clf_used.predict_proba(X_dev)\nexcept Exception:\n    from scipy.special import expit\n    dev_probas = np.vstack([est.predict_proba(tfidf_used.transform(X_dev))[:,1] for est in clf_used.estimators_]).T\n\nn_labels = dev_probas.shape[1]\nbest_thrs = np.zeros(n_labels, dtype=float)\n\n# search thresholds on dev for each label\nfor j in range(n_labels):\n    best_f1 = -1.0\n    best_t = 0.5\n    scores_j = dev_probas[:, j]\n    y_true_j = y_dev[:, j]\n    # grid search (coarse)\n    for t in np.linspace(0.05, 0.95, 19):\n        preds_j = (scores_j >= t).astype(int)\n        f1j = f1_score(y_true_j, preds_j, zero_division=0)\n        if f1j > best_f1:\n            best_f1 = f1j\n            best_t = t\n    best_thrs[j] = best_t\n\n# evaluate with per-label thresholds\npreds_dev_opt = (dev_probas >= best_thrs.reshape(1, -1)).astype(int)\nmicro_opt = f1_score(y_dev.reshape(-1), preds_dev_opt.reshape(-1), average=\"micro\", zero_division=0)\nmacro_opt = f1_score(y_dev.reshape(-1), preds_dev_opt.reshape(-1), average=\"macro\", zero_division=0)\navg_pos = preds_dev_opt.sum()/preds_dev_opt.shape[0]\n\nprint(f\"Per-label tuned -> dev micro-F1: {micro_opt:.4f} | macro-F1: {macro_opt:.4f} | avg positives/sample: {avg_pos:.3f}\")\nprint(\"First 10 per-label thresholds:\", [(i, emotions[i], float(best_thrs[i])) for i in range(10)])\n\n# save thresholds and pipeline\nnp.save(\"/kaggle/working/goemotions_per_label_thresholds.npy\", best_thrs)\njoblib.dump(clf_used, \"/kaggle/working/goemotions_clf.joblib\")\njoblib.dump(tfidf_used, \"/kaggle/working/goemotions_tfidf.joblib\")\nprint(\"Saved thresholds + pipeline pieces to /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:09:54.823018Z","iopub.execute_input":"2025-10-26T15:09:54.823294Z","iopub.status.idle":"2025-10-26T15:09:56.007172Z","shell.execute_reply.started":"2025-10-26T15:09:54.823276Z","shell.execute_reply":"2025-10-26T15:09:56.006449Z"}},"outputs":[{"name":"stdout","text":"Using pipeline2 (improved).\nPer-label tuned -> dev micro-F1: 0.8977 | macro-F1: 0.6121 | avg positives/sample: 2.800\nFirst 10 per-label thresholds: [(0, 'admiration', 0.15), (1, 'amusement', 0.1), (2, 'anger', 0.05), (3, 'annoyance', 0.1), (4, 'approval', 0.05), (5, 'caring', 0.05), (6, 'confusion', 0.05), (7, 'curiosity', 0.05), (8, 'desire', 0.05), (9, 'disappointment', 0.05)]\nSaved thresholds + pipeline pieces to /kaggle/working/\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# 11_predict_single_sentences_with_thresholds\nimport numpy as np\nimport joblib\n\n# load pipeline pieces if not in memory\ntry:\n    clf_used\n    tfidf_used\nexcept NameError:\n    tfidf_used = joblib.load(\"/kaggle/working/goemotions_tfidf.joblib\")\n    clf_used = joblib.load(\"/kaggle/working/goemotions_clf.joblib\")\n\n# load thresholds if available, else set global thr\ntry:\n    best_thrs = np.load(\"/kaggle/working/goemotions_per_label_thresholds.npy\")\n    print(\"Loaded per-label thresholds.\")\nexcept Exception:\n    best_thrs = None\n    print(\"Per-label thresholds not found; will use global threshold.\")\n\ndef predict_texts(texts, threshold=None):\n    # texts: list[str] or single str\n    single = False\n    if isinstance(texts, str):\n        texts = [texts]; single = True\n    Xvec = tfidf_used.transform(texts)\n    # compute probs per label\n    try:\n        probas = clf_used.predict_proba(texts)  # if estimator accepts raw texts\n    except Exception:\n        # fallback: per-estimator proba using transformed features\n        probas = np.vstack([est.predict_proba(Xvec)[:,1] for est in clf_used.estimators_]).T\n    if best_thrs is not None:\n        thr = best_thrs\n    elif threshold is not None:\n        thr = np.ones(probas.shape[1]) * threshold\n    else:\n        thr = np.ones(probas.shape[1]) * 0.30  # default global threshold\n    preds = (probas >= thr.reshape(1, -1)).astype(int)\n    inv = {i: lab for i, lab in enumerate(emotions)}\n    results = []\n    for i, t in enumerate(texts):\n        label_idxs = np.where(preds[i] == 1)[0].tolist()\n        labels = [inv[idx] for idx in label_idxs]\n        probs = {inv[j]: float(probas[i, j]) for j in label_idxs}\n        results.append({\"text\": t, \"pred_labels\": labels, \"pred_probs\": probs})\n    return results[0] if single else results\n\n# examples\nprint(predict_texts(\"I have faith\"))\nprint(predict_texts(\"Haha, my apologies!\"))\nprint(predict_texts(\"I am furious and disgusted\"))\n# you can call predict_texts(\"your sentence here\", threshold=0.25) to override\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:10:17.630975Z","iopub.execute_input":"2025-10-26T15:10:17.631764Z","iopub.status.idle":"2025-10-26T15:10:17.651727Z","shell.execute_reply.started":"2025-10-26T15:10:17.631740Z","shell.execute_reply":"2025-10-26T15:10:17.650898Z"}},"outputs":[{"name":"stdout","text":"Loaded per-label thresholds.\n{'text': 'I have faith', 'pred_labels': ['approval', 'curiosity', 'neutral'], 'pred_probs': {'approval': 0.05340669138068469, 'curiosity': 0.05472486367480756, 'neutral': 0.3627720442782259}}\n{'text': 'Haha, my apologies!', 'pred_labels': ['amusement', 'approval'], 'pred_probs': {'amusement': 0.10223449950155794, 'approval': 0.05769728243428152}}\n{'text': 'I am furious and disgusted', 'pred_labels': ['approval', 'curiosity'], 'pred_probs': {'approval': 0.05757634854339065, 'curiosity': 0.05382702505079021}}\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# 12_zip_artifacts_for_download\nimport shutil, os\nout_dir = \"/kaggle/working/goemotions_artifacts\"\nos.makedirs(out_dir, exist_ok=True)\n\n# copy relevant files\nfor f in [\n    \"/kaggle/working/goemotions_tfidf_pipeline2.joblib\",\n    \"/kaggle/working/goemotions_clf.joblib\",\n    \"/kaggle/working/goemotions_tfidf.joblib\",\n    \"/kaggle/working/goemotions_per_label_thresholds.npy\"\n]:\n    if os.path.exists(f):\n        shutil.copy(f, out_dir)\n\n!zip -r /kaggle/working/goemotions_artifacts.zip /kaggle/working/goemotions_artifacts\nprint(\"Zipped artifacts to /kaggle/working/goemotions_artifacts.zip (see Output tab)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:11:24.429580Z","iopub.execute_input":"2025-10-26T15:11:24.429857Z","iopub.status.idle":"2025-10-26T15:11:24.900928Z","shell.execute_reply.started":"2025-10-26T15:11:24.429840Z","shell.execute_reply":"2025-10-26T15:11:24.899830Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/goemotions_artifacts/ (stored 0%)\n  adding: kaggle/working/goemotions_artifacts/goemotions_tfidf.joblib (deflated 77%)\n  adding: kaggle/working/goemotions_artifacts/goemotions_clf.joblib","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 69%)\n  adding: kaggle/working/goemotions_artifacts/goemotions_tfidf_pipeline2.joblib (deflated 70%)\n  adding: kaggle/working/goemotions_artifacts/goemotions_per_label_thresholds.npy (deflated 74%)\nZipped artifacts to /kaggle/working/goemotions_artifacts.zip (see Output tab)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# 13_finalize_and_zip_output\nimport shutil, os, zipfile\n\nOUTDIR = \"/kaggle/working/goemotions_final\"\nos.makedirs(OUTDIR, exist_ok=True)\n\n# copy your key files\nfor fname in [\n    \"goemotions_tfidf_baseline.joblib\",\n    \"goemotions_tfidf_pipeline2.joblib\",\n    \"goemotions_clf.joblib\",\n    \"goemotions_tfidf.joblib\",\n    \"goemotions_per_label_thresholds.npy\"\n]:\n    src = f\"/kaggle/working/{fname}\"\n    if os.path.exists(src):\n        shutil.copy(src, OUTDIR)\n\n# zip it\nzip_path = \"/kaggle/working/goemotions_final.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUTDIR):\n        for f in files:\n            full_path = os.path.join(root, f)\n            zf.write(full_path, os.path.relpath(full_path, OUTDIR))\n\nprint(f\"✅ Zipped everything to {zip_path}\\nCheck the 'Output' sidebar → goemotions_final.zip → Download\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:14:26.867143Z","iopub.execute_input":"2025-10-26T15:14:26.868048Z","iopub.status.idle":"2025-10-26T15:14:27.225825Z","shell.execute_reply.started":"2025-10-26T15:14:26.868020Z","shell.execute_reply":"2025-10-26T15:14:27.225046Z"}},"outputs":[{"name":"stdout","text":"✅ Zipped everything to /kaggle/working/goemotions_final.zip\nCheck the 'Output' sidebar → goemotions_final.zip → Download\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}